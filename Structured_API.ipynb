{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification with Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1.1 Structured API Implementation (High-Level)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CreditCardFraud\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"creditcard.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Data Preprocessing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains credit card transactions over two days in 2013, with a severe class imbalance: only 0.172% of the transactions are fraudulent (492 out of 284,807).\n",
    "\n",
    "### Key Points About the Data:\n",
    "- Features V1 to V28 are PCA-transformed, anonymized components.\n",
    "\n",
    "- Time shows seconds since the first transaction — not very useful for fraud prediction.\n",
    "\n",
    "- Amount is the raw transaction value — not standardized, unlike the PCA features.\n",
    "\n",
    "- Class is the target label — 1 for fraud, 0 for normal.\n",
    "\n",
    "### Preprocessing Steps:\n",
    "\n",
    "- Ensure data quality by removing rows with any missing/null values.\n",
    "\n",
    "- Standardize Amount since Amount is on a different scale than the PCA components, we standardize it for consistency.\n",
    "\n",
    "- Combine all numerical features (including the scaled Amount) into a single feature vector, as required by machine learning algorithms in Spark.\n",
    "\n",
    "- Use the Class column as the label for classification — 1 (fraud), 0 (non-fraud).\n",
    "\n",
    "- Apply undersampling to address class imbalance. Since the number of fraud cases is much lower, we randomly sample from the majority class (non-fraud) to match the number of fraud samples. This results in a balanced dataset that helps prevent the model from being biased toward the majority class.\n",
    "\n",
    "- Because fraud cases are rare, metrics like precision, recall, and AUC-PR are more appropriate than accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|Time| V1| V2| V3| V4| V5| V6| V7| V8| V9|V10|V11|V12|V13|V14|V15|V16|V17|V18|V19|V20|V21|V22|V23|V24|V25|V26|V27|V28|Amount|Class|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     0|    0|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "pca_features = [c for c in df.columns if c.startswith(\"V\")]  # V1 to V28\n",
    "feature_cols_to_scale = [\"Amount\"]\n",
    "final_feature_cols = pca_features + feature_cols_to_scale\n",
    "\n",
    "# Assemble Amount for scaling\n",
    "assembler_for_scaling = VectorAssembler(inputCols=feature_cols_to_scale, outputCol=\"amount_vec\")\n",
    "df = assembler_for_scaling.transform(df)\n",
    "\n",
    "# Scale Amount\n",
    "scaler = StandardScaler(inputCol=\"amount_vec\", outputCol=\"scaled_amount\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(df)\n",
    "df = scaler_model.transform(df)\n",
    "\n",
    "# Drop raw Amount and rename scaled column\n",
    "df = df.drop(\"Amount\", \"amount_vec\")\n",
    "df = df.withColumnRenamed(\"scaled_amount\", \"Amount\")\n",
    "\n",
    "# Separate classes\n",
    "class_1_df = df.filter(col(\"Class\") == 1)\n",
    "class_0_df = df.filter(col(\"Class\") == 0)\n",
    "\n",
    "# Match number of class 0 to class 1\n",
    "count_1 = class_1_df.count()\n",
    "balanced_0_df = class_0_df.sample(False, fraction=(count_1 / class_0_df.count()), seed=2505)\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced_df = balanced_0_df.union(class_1_df)\n",
    "balanced_df = balanced_df.orderBy(rand(seed = 2505))\n",
    "\n",
    "\n",
    "input_columns = [col_name for col_name in balanced_df.columns if col_name != \"Class\"]\n",
    "\n",
    "data = VectorAssembler(inputCols=input_columns, outputCol=\"Features\") \\\n",
    "           .transform(balanced_df).select(\"Features\", col(\"Class\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Train the Logistic Regression model using MLlib:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 619:==============================>                       (18 + 14) / 32]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"Features\", labelCol=\"Class\")\n",
    "\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 645:==============================>                       (18 + 14) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9350\n",
      "AUC: 0.9643\n",
      "Precision: 0.9361\n",
      "Recall: 0.9350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "accuracy = MulticlassClassificationEvaluator(labelCol=\"Class\", metricName=\"accuracy\").evaluate(predictions)\n",
    "\n",
    "auc = BinaryClassificationEvaluator(labelCol=\"Class\", metricName=\"areaUnderROC\").evaluate(predictions)\n",
    "\n",
    "precision = MulticlassClassificationEvaluator(labelCol=\"Class\", metricName=\"weightedPrecision\").evaluate(predictions)\n",
    "\n",
    "recall = MulticlassClassificationEvaluator(labelCol=\"Class\", metricName=\"weightedRecall\").evaluate(predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
