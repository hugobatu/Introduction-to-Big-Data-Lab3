{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib RDD-Based Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LogisticRegression_Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"creditcard.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains credit card transactions over two days in 2013, with a severe class imbalance: only 0.172% of the transactions are fraudulent (492 out of 284,807).\n",
    "\n",
    "### Key Points About the Data:\n",
    "- Features V1 to V28 are PCA-transformed, anonymized components.\n",
    "\n",
    "- Time shows seconds since the first transaction — not very useful for fraud prediction.\n",
    "\n",
    "- Amount is the raw transaction value — not standardized, unlike the PCA features.\n",
    "\n",
    "- Class is the target label — 1 for fraud, 0 for normal.\n",
    "\n",
    "### Preprocessing Steps:\n",
    "\n",
    "- Ensure data quality by removing rows with any missing/null values.\n",
    "\n",
    "- Standardize Amount since Amount is on a different scale than the PCA components, we standardize it for consistency.\n",
    "\n",
    "- Combine all numerical features (including the scaled Amount) into a single feature vector, as required by machine learning algorithms in Spark.\n",
    "\n",
    "- Use the Class column as the label for classification — 1 (fraud), 0 (non-fraud).\n",
    "\n",
    "- Because fraud cases are rare, metrics like precision, recall, and AUC-PR are more appropriate than accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|Time| V1| V2| V3| V4| V5| V6| V7| V8| V9|V10|V11|V12|V13|V14|V15|V16|V17|V18|V19|V20|V21|V22|V23|V24|V25|V26|V27|V28|Amount|Class|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     0|    0|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "pca_features = [c for c in df.columns if c.startswith(\"V\")]  # V1 to V28\n",
    "feature_cols_to_scale = [\"Amount\"]\n",
    "final_feature_cols = pca_features + feature_cols_to_scale\n",
    "\n",
    "assembler_for_scaling = VectorAssembler(inputCols=feature_cols_to_scale, outputCol=\"amount_vec\")\n",
    "df = assembler_for_scaling.transform(df)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"amount_vec\", outputCol=\"scaled_amount\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(df)\n",
    "df = scaler_model.transform(df)\n",
    "\n",
    "df = df.drop(\"Amount\", \"amount_vec\")\n",
    "df = df.withColumnRenamed(\"scaled_amount\", \"Amount\")\n",
    "\n",
    "final_features = pca_features + [\"Amount\"]\n",
    "assembler = VectorAssembler(inputCols=final_features, outputCol=\"features\")\n",
    "df = assembler.transform(df).select(\"features\", col(\"Class\").alias(\"label\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLlib RDD-Based Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 16:40:01 WARN Instrumentation: [c8042e57] Initial coefficients will be ignored! Its dimensions (1, 29) did not match the expected size (1, 29)\n",
      "[Stage 310:========================>                               (7 + 9) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MLlib RDD-Based Evaluation Results ===\n",
      "Total Test Samples: 57105\n",
      "Accuracy:  0.9993\n",
      "Precision: 0.8230\n",
      "Recall:    0.8378\n",
      "F1 Score:  0.8304\n",
      "Mean Squared Error (MSE): 0.000665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "\n",
    "rdd_data = df.rdd.map(lambda row: LabeledPoint(row[\"label\"], Vectors.dense(row[\"features\"])))\n",
    "\n",
    "train_rdd, test_rdd = rdd_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model_rdd = LogisticRegressionWithLBFGS.train(train_rdd, iterations=100, numClasses=2)\n",
    "\n",
    "\n",
    "predictions_rdd = test_rdd.map(lambda p: (float(model_rdd.predict(p.features)), p.label))\n",
    "\n",
    "\n",
    "correct_preds = predictions_rdd.filter(lambda x: x[0] == x[1]).count()\n",
    "total_preds = test_rdd.count()\n",
    "accuracy_rdd = correct_preds / float(total_preds)\n",
    "\n",
    "\n",
    "# TP = predicted 1, actual 1\n",
    "tp = predictions_rdd.filter(lambda x: x == (1.0, 1.0)).count()\n",
    "# FP = predicted 1, actual 0\n",
    "fp = predictions_rdd.filter(lambda x: x == (1.0, 0.0)).count()\n",
    "# FN = predicted 0, actual 1\n",
    "fn = predictions_rdd.filter(lambda x: x == (0.0, 1.0)).count()\n",
    "\n",
    "# Precision = TP / (TP + FP)\n",
    "precision_rdd = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "# Recall = TP / (TP + FN)\n",
    "recall_rdd = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "# Square of (prediction - actual) averaged over all samples\n",
    "mse_rdd = predictions_rdd.map(lambda x: (x[0] - x[1]) ** 2).mean()\n",
    "\n",
    "f1_rdd = 2 * precision_rdd * recall_rdd / (precision_rdd + recall_rdd) if (precision_rdd + recall_rdd) != 0 else 0\n",
    "\n",
    "print(\"\\n=== MLlib RDD-Based Evaluation Results ===\")\n",
    "print(f\"Total Test Samples: {total_preds}\")\n",
    "print(f\"Accuracy:  {accuracy_rdd:.4f}\")\n",
    "print(f\"Precision: {precision_rdd:.4f}\")\n",
    "print(f\"Recall:    {recall_rdd:.4f}\")\n",
    "print(f\"F1 Score:  {f1_rdd:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_rdd:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
