{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T16:16:46.389111Z",
     "start_time": "2025-04-12T16:16:45.254971Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour, dayofweek, month, radians, asin, sqrt, pow, dayofyear, log, exp, sin, cos, col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:16:59.906701Z",
     "start_time": "2025-04-12T16:16:46.439756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Problem2 Structured API\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.log.level\", \"ERROR\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "2df60a0463a0ca07",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/12 23:16:53 WARN Utils: Your hostname, nam-Nitro-AN515-45 resolves to a loopback address: 127.0.1.1; using 192.168.1.18 instead (on interface wlp5s0)\n",
      "25/04/12 23:16:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/12 23:16:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting Spark log level to \"ERROR\".\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:17:17.131653Z",
     "start_time": "2025-04-12T16:17:00.487966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "try:\n",
    "    train_data = spark.read.csv(\"train.csv\", header=True, inferSchema=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading train data: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)"
   ],
   "id": "e2b79c7f9072cbd3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:17:17.149724Z",
     "start_time": "2025-04-12T16:17:17.139351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature engineering function\n",
    "def prepare_features(data):\n",
    "    return data.withColumn(\"pickup_day_of_year\", dayofyear(\"pickup_datetime\")) \\\n",
    "        .withColumn(\"pickup_day_of_week\", dayofweek(\"pickup_datetime\")) \\\n",
    "        .withColumn(\"pickup_hour_of_day\", hour(\"pickup_datetime\")) \\\n",
    "        .withColumn(\"pickup_month\", month(\"pickup_datetime\")) \\\n",
    "        .withColumn(\"haversine\",\n",
    "                    6371 * 2 * asin(sqrt(pow(sin((radians(\"pickup_latitude\") - radians(\"dropoff_latitude\")) / 2), 2) +\n",
    "                                         cos(radians(\"pickup_latitude\")) * cos(radians(\"dropoff_latitude\")) *\n",
    "                                         pow(sin((radians(\"pickup_longitude\") - radians(\"dropoff_longitude\")) / 2),\n",
    "                                             2))))\n",
    "\n",
    "\n",
    "# Data cleaning and outlier removal using IQR\n",
    "def remove_outliers(data, column, lower_quantile=0.25, upper_quantile=0.75, k=1.5):\n",
    "    quantiles = data.approxQuantile(column, [lower_quantile, upper_quantile], 0.05)\n",
    "    iqr = quantiles[1] - quantiles[0]\n",
    "    lower_bound = quantiles[0] - k * iqr\n",
    "    upper_bound = quantiles[1] + k * iqr\n",
    "    return data.filter((col(column) >= lower_bound) & (col(column) <= upper_bound))\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(predictions, dataset_name):\n",
    "    predictions = predictions.withColumn(\"predicted_duration\", col(\"prediction\"))\n",
    "    metrics = {}\n",
    "    for metric in [\"rmse\", \"mse\", \"mae\", \"r2\"]:\n",
    "        evaluator = RegressionEvaluator(labelCol=\"trip_duration\", predictionCol=\"predicted_duration\", metricName=metric)\n",
    "        value = evaluator.evaluate(predictions)\n",
    "        metrics[metric] = value\n",
    "        print(f\"\\n{metric.upper()} on {dataset_name} = {value}\")\n",
    "    return metrics"
   ],
   "id": "42a8ce330cb37b22",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:17:26.703891Z",
     "start_time": "2025-04-12T16:17:17.193823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Features\n",
    "train_data = prepare_features(train_data)\n",
    "train_data.cache()\n",
    "\n",
    "# Remove outliers\n",
    "train_data = train_data.filter(\"haversine > 0\").filter(\"passenger_count > 0\").filter(\"trip_duration > 0\")\n",
    "train_data = remove_outliers(train_data, \"trip_duration\")\n",
    "train_data = remove_outliers(train_data, \"haversine\")"
   ],
   "id": "d67eb1d9d43936e1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:17:27.102928Z",
     "start_time": "2025-04-12T16:17:26.726111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "            \"pickup_day_of_year\", \"pickup_day_of_week\", \"pickup_hour_of_day\", \"pickup_month\",\n",
    "            \"haversine\", \"passenger_count\"]\n",
    "\n",
    "# VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(train_data).select(\"id\", \"features\", \"trip_duration\")\n",
    "assembled_data.cache()"
   ],
   "id": "53cca8c09abc4dfe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, features: vector, trip_duration: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:17:27.182754Z",
     "start_time": "2025-04-12T16:17:27.132448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Decision Tree Regressor\n",
    "decision_tree = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"trip_duration\", seed=42)\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(decision_tree.maxDepth, [5]) \\\n",
    "    .addGrid(decision_tree.minInstancesPerNode, [5]) \\\n",
    "    .build()\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"trip_duration\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Set up CrossValidator\n",
    "crossval = CrossValidator(estimator=decision_tree,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5,\n",
    "                          seed=42)"
   ],
   "id": "97831c7c259e7302",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:17:47.998379Z",
     "start_time": "2025-04-12T16:17:27.192289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train model\n",
    "try:\n",
    "    cv_model = crossval.fit(assembled_data)\n",
    "    best_model = cv_model.bestModel\n",
    "except Exception as e:\n",
    "    print(f\"Error during model training: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Split into training and validation sets for final evaluation\n",
    "train, validation = assembled_data.randomSplit([0.8, 0.2], seed=42)"
   ],
   "id": "63289073d6dad7df",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:17:53.688746Z",
     "start_time": "2025-04-12T16:17:48.088717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Predictions and evaluation\n",
    "train_predictions = best_model.transform(train)\n",
    "print(\"\\n=== Training Data Metrics ===\")\n",
    "train_metrics = evaluate_model(train_predictions, \"training data\")\n",
    "\n",
    "validation_predictions = best_model.transform(validation)\n",
    "print(\"\\n=== Validation Data Metrics ===\")\n",
    "validation_metrics = evaluate_model(validation_predictions, \"validation data\")"
   ],
   "id": "84c82d8318d2b96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Data Metrics ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE on training data = 256.6838576839673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE on training data = 65886.60279552318\n",
      "\n",
      "MAE on training data = 192.12153839895186\n",
      "\n",
      "R2 on training data = 0.519306594038818\n",
      "\n",
      "=== Validation Data Metrics ===\n",
      "\n",
      "RMSE on validation data = 257.5303382548545\n",
      "\n",
      "MSE on validation data = 66321.87512165976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAE on validation data = 192.72449711452396\n",
      "\n",
      "R2 on validation data = 0.5194423380033124\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:18:11.109833Z",
     "start_time": "2025-04-12T16:18:03.096570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model diagnostics\n",
    "print(\"\\n=== Model Diagnostics ===\")\n",
    "print(\"Feature Importances:\")\n",
    "for i, importance in enumerate(best_model.featureImportances):\n",
    "    print(f\"{features[i]}: {importance}\")\n",
    "\n",
    "print(\"Decision Tree Depth:\", best_model.depth)\n",
    "print(\"Number of Nodes:\", best_model.numNodes)\n",
    "print(\"Number of Features:\", best_model.numFeatures)\n",
    "print(\"Best maxDepth:\", best_model.getMaxDepth())\n",
    "print(\"Best minInstancesPerNode:\", best_model.getMinInstancesPerNode())\n",
    "\n",
    "try:\n",
    "    test_data = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading test CSV file: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "test_data = prepare_features(test_data)\n",
    "\n",
    "# test_data = test_data.filter(\"haversine > 0\").filter(\"passenger_count > 0\")\n",
    "\n",
    "test_assembled = assembler.transform(test_data).select(\"id\", \"features\")\n",
    "\n",
    "test_predictions = best_model.transform(test_assembled)\n",
    "\n",
    "test_predictions = test_predictions.withColumn(\"trip_duration\", col(\"prediction\"))\n",
    "\n",
    "test_output = test_predictions.select(\"id\", \"trip_duration\")\n",
    "\n",
    "try:\n",
    "    test_output.coalesce(1).write.csv(\"test_predictions.csv\", header=True, mode=\"overwrite\")\n",
    "    print(\"Predictions saved to test_predictions.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving predictions: {e}\")\n",
    "\n",
    "train_data.unpersist()\n",
    "assembled_data.unpersist()\n",
    "spark.stop()"
   ],
   "id": "3c72effb793f95c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Diagnostics ===\n",
      "Feature Importances:\n",
      "pickup_longitude: 0.0\n",
      "pickup_latitude: 0.0\n",
      "dropoff_longitude: 0.0\n",
      "dropoff_latitude: 0.03354617218304917\n",
      "pickup_day_of_year: 0.0\n",
      "pickup_day_of_week: 0.0\n",
      "pickup_hour_of_day: 0.09494252335234053\n",
      "pickup_month: 0.0\n",
      "haversine: 0.8715113044646103\n",
      "passenger_count: 0.0\n",
      "Decision Tree Depth: 5\n",
      "Number of Nodes: 63\n",
      "Number of Features: 10\n",
      "Best maxDepth: 5\n",
      "Best minInstancesPerNode: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to test_predictions.csv\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
