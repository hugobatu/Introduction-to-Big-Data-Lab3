{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-11T08:21:14.562222Z",
     "start_time": "2025-04-11T08:21:14.254430Z"
    }
   },
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.linalg import Vectors"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T08:21:17.121190Z",
     "start_time": "2025-04-11T08:21:14.575071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Spark context\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")"
   ],
   "id": "8721563a57d54ad3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/11 15:21:15 WARN Utils: Your hostname, nam-Nitro-AN515-45 resolves to a loopback address: 127.0.1.1; using 192.168.1.18 instead (on interface wlp5s0)\n",
      "25/04/11 15:21:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/11 15:21:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T08:21:17.551677Z",
     "start_time": "2025-04-11T08:21:17.232660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    data = sc.textFile(\"train.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV file: {e}\")\n",
    "    sc.stop()\n",
    "    exit(1)"
   ],
   "id": "a315978fcf3898dd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T08:21:17.563213Z",
     "start_time": "2025-04-11T08:21:17.559171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parse the dataset\n",
    "def parse_datetime(datetime_str):\n",
    "    dt = datetime.strptime(datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return [dt.hour, dt.weekday(), dt.month, dt.timetuple().tm_yday]\n",
    "\n",
    "def compute_distance(long_1, lat_1, long_2, lat_2):\n",
    "    long_diff = math.radians(long_1 - long_2) / 2\n",
    "    lat_diff = math.radians(lat_1 - lat_2) / 2\n",
    "    a = math.sin(lat_diff) ** 2 + \\\n",
    "        math.cos(math.radians(lat_1)) * \\\n",
    "        math.cos(math.radians(lat_2)) * \\\n",
    "        math.sin(long_diff) ** 2\n",
    "    return 6371 * 2 * math.asin(math.sqrt(a))\n",
    "\n",
    "def parse(row):\n",
    "    cols = row.split(\",\")\n",
    "    passenger_count = float(cols[4])\n",
    "    long_lats = [float(col) for col in cols[5:9]]\n",
    "    trip_duration = float(cols[10])\n",
    "    pickup_datetime = parse_datetime(cols[2])\n",
    "    trip_distance = compute_distance(*long_lats)\n",
    "    # Features: hour, weekday, month, day_of_year, passenger_count, pickup_long, pickup_lat, dropoff_long, dropoff_lat, distance\n",
    "    # Target: trip_duration\n",
    "    return [*pickup_datetime, passenger_count, *long_lats, trip_distance, trip_duration]"
   ],
   "id": "57d7750497ac4aa9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T08:21:18.532626Z",
     "start_time": "2025-04-11T08:21:17.609768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "header = data.first()\n",
    "data = data.filter(lambda row: row != header)\n",
    "data = data.map(parse)\n",
    "\n",
    "# Initial filtering\n",
    "data = data.filter(lambda row: row[9] > 0)  # distance > 0\n",
    "data = data.filter(lambda row: row[10] > 0)  # duration > 0\n",
    "data = data.filter(lambda row: row[4] > 0)  # passenger_count > 0"
   ],
   "id": "50252c5690be196c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T08:21:27.503733Z",
     "start_time": "2025-04-11T08:21:18.539115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess the data\n",
    "def remove_outliers(data, column, lower_quantile=0.25, upper_quantile=0.75, k=1.5):\n",
    "    values = data.map(lambda row: row[column]).collect()\n",
    "    lower_quantile_value = np.percentile(values, lower_quantile * 100)\n",
    "    upper_quantile_value = np.percentile(values, upper_quantile * 100)\n",
    "    iqr = upper_quantile_value - lower_quantile_value\n",
    "    lower_bound = lower_quantile_value - k * iqr\n",
    "    upper_bound = upper_quantile_value + k * iqr\n",
    "    return data.filter(lambda row: lower_bound <= row[column] <= upper_bound)\n",
    "\n",
    "data = remove_outliers(data, 9)  # distance\n",
    "data = remove_outliers(data, 10)  # duration"
   ],
   "id": "f0112ff3010f4527",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T08:21:42.698335Z",
     "start_time": "2025-04-11T08:21:27.514094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add log-transformed target\n",
    "data = data.map(lambda row: (*row[:-1], np.log(row[-1]), row[-1]))  # (features, log_duration, original_duration)\n",
    "\n",
    "# Convert to (LabeledPoint, original_duration) tuple and split train/val\n",
    "data = data.map(lambda row: (LabeledPoint(row[-2], row[:-2]), row[-1]))\n",
    "train_data, val_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = DecisionTree.trainRegressor(\n",
    "    train_data.map(lambda x: x[0]),  # Extract LabeledPoint for training\n",
    "    categoricalFeaturesInfo={},\n",
    "    maxDepth=10,\n",
    ")"
   ],
   "id": "81e0bb27f849458",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T08:21:42.748962Z",
     "start_time": "2025-04-11T08:21:42.741455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, data, dataset_name):\n",
    "    # Extract features, log labels, and original labels\n",
    "    features = data.map(lambda x: x[0].features)\n",
    "    log_labels = data.map(lambda x: x[0].label)  # log(trip_duration)\n",
    "    original_labels = data.map(lambda x: x[1])   # original trip_duration\n",
    "\n",
    "    # Get predictions in log space\n",
    "    log_predictions = model.predict(features)\n",
    "\n",
    "    # Compute metrics in log scale\n",
    "    log_pred_and_labels = log_predictions.zip(log_labels).map(lambda x: (float(x[0]), float(x[1])))\n",
    "    log_metrics = RegressionMetrics(log_pred_and_labels)\n",
    "\n",
    "    # Exponentiate predictions to original scale\n",
    "    predictions = log_predictions.map(lambda x: math.exp(x))\n",
    "    pred_and_labels = predictions.zip(original_labels).map(lambda x: (float(x[0]), float(x[1])))\n",
    "    metrics = RegressionMetrics(pred_and_labels)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n=== {dataset_name} Metrics ===\")\n",
    "    print(\"Log Scale:\")\n",
    "    print(f\"RMSE: {log_metrics.rootMeanSquaredError}\")\n",
    "    print(\"Original Scale:\")\n",
    "    print(f\"RMSE: {metrics.rootMeanSquaredError}\")\n",
    "    print(f\"MSE: {metrics.meanSquaredError}\")\n",
    "    print(f\"MAE: {metrics.meanAbsoluteError}\")\n",
    "    print(f\"R2: {metrics.r2}\")"
   ],
   "id": "1043701d8ae07ff8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T08:23:17.057360Z",
     "start_time": "2025-04-11T08:21:42.784327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluate_model(model, train_data, \"Training Data\")\n",
    "evaluate_model(model, val_data, \"Validation Data\")\n",
    "\n",
    "sc.stop()"
   ],
   "id": "e80529ad9fb82d4f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):                                  (0 + 1) / 1]\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):                                  (0 + 1) / 1]\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Data Metrics ===\n",
      "Log Scale:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.3995465328271772\n",
      "Original Scale:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 232.81528966695512\n",
      "MSE: 54202.95910270822\n",
      "MAE: 168.98364250228698\n",
      "R2: 0.6150950568141453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                  (0 + 1) / 1]\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation Data Metrics ===\n",
      "Log Scale:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.40218132051070504\n",
      "Original Scale:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 233.34923820814046\n",
      "MSE: 54451.86697231948\n",
      "MAE: 169.78038293067638\n",
      "R2: 0.6138538894816835\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
