{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-11T17:39:06.304183Z",
     "start_time": "2025-04-11T17:39:06.126655Z"
    }
   },
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.linalg import Vectors"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:39:08.434833Z",
     "start_time": "2025-04-11T17:39:06.307909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Spark context\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")"
   ],
   "id": "8721563a57d54ad3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/12 00:39:07 WARN Utils: Your hostname, nam-Nitro-AN515-45 resolves to a loopback address: 127.0.1.1; using 192.168.1.18 instead (on interface wlp5s0)\n",
      "25/04/12 00:39:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/12 00:39:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:39:08.798165Z",
     "start_time": "2025-04-11T17:39:08.529036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    data = sc.textFile(\"train.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV file: {e}\")\n",
    "    sc.stop()\n",
    "    exit(1)"
   ],
   "id": "a315978fcf3898dd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:39:08.808204Z",
     "start_time": "2025-04-11T17:39:08.804480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parse the dataset\n",
    "def parse_datetime(datetime_str):\n",
    "    dt = datetime.strptime(datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return [dt.hour, dt.weekday(), dt.month, dt.timetuple().tm_yday]\n",
    "\n",
    "\n",
    "def compute_distance(long_1, lat_1, long_2, lat_2):\n",
    "    long_diff = math.radians(long_1 - long_2) / 2\n",
    "    lat_diff = math.radians(lat_1 - lat_2) / 2\n",
    "    a = math.sin(lat_diff) ** 2 + \\\n",
    "        math.cos(math.radians(lat_1)) * \\\n",
    "        math.cos(math.radians(lat_2)) * \\\n",
    "        math.sin(long_diff) ** 2\n",
    "    return 6371 * 2 * math.asin(math.sqrt(a))\n",
    "\n",
    "\n",
    "def parse(row):\n",
    "    cols = row.split(\",\")\n",
    "    passenger_count = float(cols[4])\n",
    "    long_lats = [float(col) for col in cols[5:9]]\n",
    "    trip_duration = float(cols[10])\n",
    "    pickup_datetime = parse_datetime(cols[2])\n",
    "    trip_distance = compute_distance(*long_lats)\n",
    "    # Features: hour, weekday, month, day_of_year, passenger_count, pickup_long, pickup_lat, dropoff_long, dropoff_lat, distance\n",
    "    # Target: trip_duration\n",
    "    return [*pickup_datetime, passenger_count, *long_lats, trip_distance, trip_duration]"
   ],
   "id": "6ab25db450b9df24",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:39:09.642777Z",
     "start_time": "2025-04-11T17:39:08.846170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "header = data.first()\n",
    "data = data.filter(lambda row: row != header)\n",
    "data = data.map(parse)\n",
    "\n",
    "# Initial filtering\n",
    "data = data.filter(lambda row: row[9] > 0)  # distance > 0\n",
    "data = data.filter(lambda row: row[10] > 0)  # duration > 0\n",
    "data = data.filter(lambda row: row[4] > 0)  # passenger_count > 0"
   ],
   "id": "50252c5690be196c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:39:17.348382Z",
     "start_time": "2025-04-11T17:39:09.653794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess the data\n",
    "def remove_outliers(data, column, lower_quantile=0.25, upper_quantile=0.75, k=1.5):\n",
    "    values = data.map(lambda row: row[column]).collect()\n",
    "    lower_quantile_value = np.percentile(values, lower_quantile * 100)\n",
    "    upper_quantile_value = np.percentile(values, upper_quantile * 100)\n",
    "    iqr = upper_quantile_value - lower_quantile_value\n",
    "    lower_bound = lower_quantile_value - k * iqr\n",
    "    upper_bound = upper_quantile_value + k * iqr\n",
    "    return data.filter(lambda row: lower_bound <= row[column] <= upper_bound)\n",
    "\n",
    "\n",
    "data = remove_outliers(data, 9)  # distance\n",
    "data = remove_outliers(data, 10)  # duration"
   ],
   "id": "fc43f00040e76561",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:39:46.568441Z",
     "start_time": "2025-04-11T17:39:17.360057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = data.map(lambda row: (*row[:-1], row[-1]))\n",
    "\n",
    "# Convert to (LabeledPoint, duration) tuple and split train/val\n",
    "data = data.map(lambda row: (LabeledPoint(row[-1], row[:-2]), row[-1]))\n",
    "train_data, val_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = DecisionTree.trainRegressor(\n",
    "    train_data.map(lambda x: x[0]),  # Extract LabeledPoint for training\n",
    "    categoricalFeaturesInfo={},\n",
    "    maxDepth=30,\n",
    "    minInstancesPerNode=30,\n",
    "    maxBins=128,\n",
    ")"
   ],
   "id": "81e0bb27f849458",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:39:46.607852Z",
     "start_time": "2025-04-11T17:39:46.604012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, data, dataset_name):\n",
    "    features = data.map(lambda x: x[0].features)\n",
    "    labels = data.map(lambda x: x[0].label)\n",
    "\n",
    "    predictions = model.predict(features)\n",
    "\n",
    "    pred_and_labels = predictions.zip(labels).map(lambda x: (float(x[0]), float(x[1])))\n",
    "    metrics = RegressionMetrics(pred_and_labels)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n=== {dataset_name} Metrics ===\")\n",
    "    print(f\"RMSE: {metrics.rootMeanSquaredError}\")\n",
    "    print(f\"MSE: {metrics.meanSquaredError}\")\n",
    "    print(f\"MAE: {metrics.meanAbsoluteError}\")\n",
    "    print(f\"R2: {metrics.r2}\")"
   ],
   "id": "1043701d8ae07ff8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T17:40:20.940900Z",
     "start_time": "2025-04-11T17:39:46.646498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluate_model(model, train_data, \"Training Data\")\n",
    "evaluate_model(model, val_data, \"Validation Data\")\n",
    "sc.stop()"
   ],
   "id": "e80529ad9fb82d4f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Data Metrics ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 197.93184847154677\n",
      "MSE: 39177.016639363355\n",
      "MAE: 146.39200471256427\n",
      "R2: 0.7217969717263647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation Data Metrics ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 217.46851064291195\n",
      "MSE: 47292.55312124631\n",
      "MAE: 160.9846768328867\n",
      "R2: 0.664624255151187\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
