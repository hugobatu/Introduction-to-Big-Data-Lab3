{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T16:15:08.622283Z",
     "start_time": "2025-04-12T16:15:08.182883Z"
    }
   },
   "source": [
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.linalg import Vectors"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "8721563a57d54ad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:15:12.235196Z",
     "start_time": "2025-04-12T16:15:08.640345Z"
    }
   },
   "source": [
    "# Spark context\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/12 23:15:10 WARN Utils: Your hostname, nam-Nitro-AN515-45 resolves to a loopback address: 127.0.1.1; using 192.168.1.18 instead (on interface wlp5s0)\n",
      "25/04/12 23:15:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/12 23:15:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/12 23:15:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a315978fcf3898dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:15:12.888545Z",
     "start_time": "2025-04-12T16:15:12.406701Z"
    }
   },
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    train_data = sc.textFile(\"train.csv\")\n",
    "    test_data = sc.textFile(\"test.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV file: {e}\")\n",
    "    sc.stop()\n",
    "    exit(1)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "6ab25db450b9df24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:15:12.903642Z",
     "start_time": "2025-04-12T16:15:12.897625Z"
    }
   },
   "source": [
    "# Parse the dataset\n",
    "def parse_datetime(datetime_str):\n",
    "    dt = datetime.strptime(datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return [dt.hour, dt.weekday(), dt.month, dt.timetuple().tm_yday]\n",
    "\n",
    "\n",
    "def compute_distance(long_1, lat_1, long_2, lat_2):\n",
    "    long_diff = math.radians(long_1 - long_2) / 2\n",
    "    lat_diff = math.radians(lat_1 - lat_2) / 2\n",
    "    a = math.sin(lat_diff) ** 2 + \\\n",
    "        math.cos(math.radians(lat_1)) * \\\n",
    "        math.cos(math.radians(lat_2)) * \\\n",
    "        math.sin(long_diff) ** 2\n",
    "    return 6371 * 2 * math.asin(math.sqrt(a))\n",
    "\n",
    "\n",
    "def parse_train(row):\n",
    "    cols = row.split(\",\")\n",
    "    passenger_count = float(cols[4])\n",
    "    long_lats = [float(col) for col in cols[5:9]]\n",
    "    trip_duration = float(cols[10])\n",
    "    pickup_datetime = parse_datetime(cols[2])\n",
    "    trip_distance = compute_distance(*long_lats)\n",
    "    # Features: hour, weekday, month, day_of_year, passenger_count, pickup_long, pickup_lat, dropoff_long, dropoff_lat, distance\n",
    "    # Target: trip_duration\n",
    "    return [*pickup_datetime, passenger_count, *long_lats, trip_distance, trip_duration]\n",
    "\n",
    "def parse_test(row):\n",
    "    cols = row.split(\",\")\n",
    "    passenger_count = float(cols[3])\n",
    "    long_lats = [float(col) for col in cols[4:8]]\n",
    "    pickup_datetime = parse_datetime(cols[2])\n",
    "    trip_distance = compute_distance(*long_lats)\n",
    "    trip_id = cols[0]\n",
    "    # Features: hour, weekday, month, day_of_year, passenger_count, pickup_long, pickup_lat, dropoff_long, dropoff_lat, distance\n",
    "    # Target: trip_duration\n",
    "    return [*pickup_datetime, passenger_count, *long_lats, trip_distance, trip_id]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "50252c5690be196c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:15:14.661817Z",
     "start_time": "2025-04-12T16:15:12.950495Z"
    }
   },
   "source": [
    "train_header = train_data.first()\n",
    "train_data = train_data.filter(lambda row: row != train_header)\n",
    "train_data = train_data.map(parse_train)\n",
    "\n",
    "test_header = test_data.first()\n",
    "test_data = test_data.filter(lambda row: row != test_header)\n",
    "test_data = test_data.map(parse_test)\n",
    "\n",
    "# Initial filtering\n",
    "train_data = train_data.filter(lambda row: row[9] > 0)  # distance > 0\n",
    "train_data = train_data.filter(lambda row: row[10] > 0)  # duration > 0\n",
    "train_data = train_data.filter(lambda row: row[4] > 0)  # passenger_count > 0"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "fc43f00040e76561",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:15:29.331933Z",
     "start_time": "2025-04-12T16:15:14.671107Z"
    }
   },
   "source": [
    "# Preprocess the data\n",
    "def remove_outliers(data, column, lower_quantile=0.25, upper_quantile=0.75, k=1.5):\n",
    "    values = data.map(lambda row: row[column]).collect()\n",
    "    lower_quantile_value = np.percentile(values, lower_quantile * 100)\n",
    "    upper_quantile_value = np.percentile(values, upper_quantile * 100)\n",
    "    iqr = upper_quantile_value - lower_quantile_value\n",
    "    lower_bound = lower_quantile_value - k * iqr\n",
    "    upper_bound = upper_quantile_value + k * iqr\n",
    "    return data.filter(lambda row: lower_bound <= row[column] <= upper_bound)\n",
    "\n",
    "\n",
    "train_data = remove_outliers(train_data, 9)  # distance\n",
    "train_data = remove_outliers(train_data, 10)  # duration"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "81e0bb27f849458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:15:53.527011Z",
     "start_time": "2025-04-12T16:15:29.363148Z"
    }
   },
   "source": [
    "# Convert to LabeledPoint(label, features) tuple and split train/val\n",
    "train_data = train_data.map(lambda row: LabeledPoint(row[-1], row[:-2]))\n",
    "train_data, val_data = train_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = DecisionTree.trainRegressor(\n",
    "    train_data,\n",
    "    categoricalFeaturesInfo={},\n",
    "    maxDepth=5,\n",
    "    minInstancesPerNode=5,\n",
    "    # maxBins=128,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "1043701d8ae07ff8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:15:53.559729Z",
     "start_time": "2025-04-12T16:15:53.553407Z"
    }
   },
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, data, dataset_name):\n",
    "    features = data.map(lambda x: x.features)\n",
    "    labels = data.map(lambda x: x.label)\n",
    "\n",
    "    predictions = model.predict(features)\n",
    "\n",
    "    pred_and_labels = predictions.zip(labels).map(lambda x: (float(x[0]), float(x[1])))\n",
    "    metrics = RegressionMetrics(pred_and_labels)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"\\n=== {dataset_name} Metrics ===\")\n",
    "    print(f\"RMSE: {metrics.rootMeanSquaredError}\")\n",
    "    print(f\"MSE: {metrics.meanSquaredError}\")\n",
    "    print(f\"MAE: {metrics.meanAbsoluteError}\")\n",
    "    print(f\"R2: {metrics.r2}\")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "e80529ad9fb82d4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:17:03.774570Z",
     "start_time": "2025-04-12T16:15:53.604822Z"
    }
   },
   "source": [
    "evaluate_model(model, train_data, \"Training Data\")\n",
    "evaluate_model(model, val_data, \"Validation Data\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):                                  (0 + 1) / 1]\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Data Metrics ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 343.5877765820923\n",
      "MSE: 118052.56021662579\n",
      "MAE: 274.8985600143492\n",
      "R2: 0.1616875769268712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):                                  (0 + 1) / 1]\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1291, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/nam/PycharmProjects/PySparkProject/.venv/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation Data Metrics ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 343.45082670929776\n",
      "MSE: 117958.47036730006\n",
      "MAE: 274.62106313363273\n",
      "R2: 0.1634959999043215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "e5fc864f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T16:17:25.738591Z",
     "start_time": "2025-04-12T16:17:04.026480Z"
    }
   },
   "source": [
    "test_features = test_data.map(lambda row: Vectors.dense(row[0:9]))\n",
    "test_ids = test_data.map(lambda row: row[-1])\n",
    "predictions = model.predict(test_features)\n",
    "result = test_ids.zip(predictions)\n",
    "result_list = result.collect()\n",
    "\n",
    "with open(\"predictions.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"trip_duration\"])\n",
    "    writer.writerows(result_list)\n",
    "sc.stop()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
