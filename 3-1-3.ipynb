{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification with Logistic Regression**\n",
    "## **3.1.3 Low-Level Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Parse dataset and create RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "spark = SparkContext.getOrCreate()\n",
    "\n",
    "raw_data = spark.textFile(\"creditcard.csv\")\n",
    "header = raw_data.first()\n",
    "data = raw_data.filter(lambda row: row != header)\n",
    "\n",
    "parsed_data = data.map(lambda line: next(csv.reader(StringIO(line))))\n",
    "\n",
    "rdd_data = parsed_data.map(lambda fields: (\n",
    "    float(fields[-1]), # class\n",
    "    [float(x) for x in fields[1:-1]] # features\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Using undersampling to balance the class label of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# separate positive and negative samples\n",
    "positive = rdd_data.filter(lambda x: x[0] == 1.0)\n",
    "negative = rdd_data.filter(lambda x: x[0] == 0.0)\n",
    "\n",
    "# count\n",
    "pos_count = positive.count()\n",
    "neg_count = negative.count()\n",
    "\n",
    "# downsample negative class\n",
    "negative_downsampled = negative.sample(False, float(pos_count) / neg_count, seed=2505)\n",
    "\n",
    "# combine and shuffle\n",
    "balanced_rdd = positive.union(negative_downsampled).repartition(4).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Implementation of logistic regression using gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_length = len(balanced_rdd.take(1)[0][1])\n",
    "weights = [0.0] * feature_length\n",
    "learning_rate = 0.0001\n",
    "iterations = 100\n",
    "\n",
    "def dot_product(w, x):\n",
    "    return sum(wi * xi for wi, xi in zip(w, x))\n",
    "\n",
    "# sigmoid function to convert score to probability\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def compute_gradient(label, features, weights):\n",
    "    prediction = sigmoid(dot_product(weights, features))\n",
    "    error = prediction - label\n",
    "    return [error * f for f in features]\n",
    "\n",
    "for i in range(iterations):\n",
    "    gradients = balanced_rdd.map(lambda x: compute_gradient(x[0], x[1], weights))\n",
    "    \n",
    "    # average gradients over all records\n",
    "    total_gradient = gradients.reduce(lambda a, b: [x + y for x, y in zip(a, b)])\n",
    "    count = balanced_rdd.count()\n",
    "    avg_gradient = [g / count for g in total_gradient]\n",
    "    \n",
    "    # weight update: w = w - learning_rate * gradient\n",
    "    weights = [w - learning_rate * g for w, g in zip(weights, avg_gradient)]\n",
    "    \n",
    "    # if i % 10 == 0:\n",
    "    #     print(f\"Iteration {i}\")\n",
    "\n",
    "# predict labels using the final weights\n",
    "def predict_label(features, weights):\n",
    "    prob = sigmoid(dot_product(weights, features))\n",
    "    return 1 if prob >= 0.5 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Model evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Accuracy: 0.8686\n"
     ]
    }
   ],
   "source": [
    "# compare low-level predictions to true labels\n",
    "predictions = balanced_rdd.map(lambda x: (x[0], predict_label(x[1], weights)))\n",
    "\n",
    "# evaluate prediction performance with accuracy\n",
    "correct = predictions.filter(lambda x: x[0] == x[1]).count()\n",
    "total = predictions.count()\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"\\nFinal Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explanations:**\n",
    "- Learning rate at 0.0001: small enough due to the high-dimensional scattered data. Larger values may cause divergence.\n",
    "- Iteration count 100: enough iteration to see convergence trends and reasonable accuracy without overfitting or too long computation time.\n",
    "### **Challenges:**\n",
    "- Implementing gradient descent manually using RDD operations require careful attention to broadcasting weights and avoiding driver-only updates.\n",
    "- Parsing and managing numerical data from text-based CSV format required custom parsing and float conversion.\n",
    "- Ensuring the sigmoid function didnâ€™t overflow required care with extreme dot product values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
