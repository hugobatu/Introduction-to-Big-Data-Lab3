{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification with Logistic Regression**\n",
    "## **3.1.3 Low-Level Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1, Parse dataset and create RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: First 5 Weights = [np.float64(-8.243472362715679e-07), np.float64(6.26002459972985e-07), np.float64(-1.2149891947561749e-06), np.float64(7.846290011748039e-07), np.float64(-5.443695709306958e-07)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: First 5 Weights = [np.float64(-4.3636329131608646e-05), np.float64(-0.00010302810510938469), np.float64(-3.172548015546557e-05), np.float64(1.1821190342231535e-05), np.float64(-6.250391452532119e-05)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20: First 5 Weights = [np.float64(-8.345356183566156e-05), np.float64(-0.0002186152784890349), np.float64(-5.7503318323527295e-05), np.float64(1.9473942054053385e-05), np.float64(-0.00013047484349576813)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30: First 5 Weights = [np.float64(-0.00012040859063773709), np.float64(-0.0003357336700380298), np.float64(-8.020200652298208e-05), np.float64(2.5557968131560203e-05), np.float64(-0.000199337867972526)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40: First 5 Weights = [np.float64(-0.00015519229214003863), np.float64(-0.00045281820453789255), np.float64(-0.00010068705562496313), np.float64(3.065613262314166e-05), np.float64(-0.00026825664664858134)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50: First 5 Weights = [np.float64(-0.00018826427222649902), np.float64(-0.0005693873354399155), np.float64(-0.00011945224579988105), np.float64(3.504986169898816e-05), np.float64(-0.0003369597224695631)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60: First 5 Weights = [np.float64(-0.0002199418457007569), np.float64(-0.0006852651071207733), np.float64(-0.00013681807496669483), np.float64(3.8903164454774705e-05), np.float64(-0.00040534335990003486)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70: First 5 Weights = [np.float64(-0.00025045470961157073), np.float64(-0.0008003887905478007), np.float64(-0.00015300975693150425), np.float64(4.2322638599734656e-05), np.float64(-0.0004733682524082145)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 80: First 5 Weights = [np.float64(-0.00027997562422709543), np.float64(-0.0009147439271618026), np.float64(-0.00016819420667226162), np.float64(4.538270617694844e-05), np.float64(-0.0005410232956085673)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 90: First 5 Weights = [np.float64(-0.00030863852366535107), np.float64(-0.0010283381974567271), np.float64(-0.0001824999627461265), np.float64(4.8138023945245084e-05), np.float64(-0.0006083105968712515)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 295:===========>                                             (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Accuracy: 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "spark = SparkContext.getOrCreate()\n",
    "\n",
    "raw_data = spark.textFile(\"creditcard.csv\")\n",
    "header = raw_data.first()\n",
    "data = raw_data.filter(lambda row: row != header)\n",
    "\n",
    "parsed_data = data.map(lambda line: next(csv.reader(StringIO(line))))\n",
    "\n",
    "rdd_data = parsed_data.map(lambda fields: (\n",
    "    float(fields[-1]), # class\n",
    "    [float(x) for x in fields[1:-1]] # features\n",
    "))\n",
    "\n",
    "# label, features = rdd_data.first()\n",
    "# print(\"\\nSample Record:\")\n",
    "# print(\"Label:\", label)\n",
    "# print(\"First 5 Features:\", features[:29])\n",
    "# print(\"Total Number of Features:\", len(features))\n",
    "\n",
    "feature_length = len(rdd_data.take(1)[0][1])\n",
    "weights = [0.0] * feature_length\n",
    "learning_rate = 0.0001\n",
    "iterations = 100\n",
    "\n",
    "\n",
    "def dot_product(w, x):\n",
    "    return sum(wi * xi for wi, xi in zip(w, x))\n",
    "\n",
    "# sigmoid function to convert score to probability\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def compute_gradient(label, features, weights):\n",
    "    prediction = sigmoid(dot_product(weights, features))\n",
    "    error = prediction - label\n",
    "    return [error * f for f in features]\n",
    "\n",
    "for i in range(iterations):\n",
    "    gradients = rdd_data.map(lambda x: compute_gradient(x[0], x[1], weights))\n",
    "    \n",
    "    # average gradients over all records\n",
    "    total_gradient = gradients.reduce(lambda a, b: [x + y for x, y in zip(a, b)])\n",
    "    count = rdd_data.count()\n",
    "    avg_gradient = [g / count for g in total_gradient]\n",
    "    \n",
    "    # weight update: w = w - learning_rate * gradient\n",
    "    weights = [w - learning_rate * g for w, g in zip(weights, avg_gradient)]\n",
    "    \n",
    "    # if i % 10 == 0:\n",
    "    #     print(f\"Iteration {i}: First 5 Weights = {weights[:5]}\")\n",
    "\n",
    "# predict labels using the final weights\n",
    "def predict_label(features, weights):\n",
    "    prob = sigmoid(dot_product(weights, features))\n",
    "    return 1 if prob >= 0.5 else 0\n",
    "\n",
    "# compare low-level predictions to true labels\n",
    "predictions = rdd_data.map(lambda x: (x[0], predict_label(x[1], weights)))\n",
    "\n",
    "# evaluate prediction performance with accuracy\n",
    "correct = predictions.filter(lambda x: x[0] == x[1]).count()\n",
    "total = predictions.count()\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"\\nFinal Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explanations:**\n",
    "- Learning rate at 0.0001: small enough due to the high-dimensional scattered data. Larger values may cause divergence.\n",
    "- Iteration count 100: enough iteration to see convergence trends and reasonable accuracy without overfitting or too long computation time.\n",
    "### **Challenges:**\n",
    "- Implementing gradient descent manually using RDD operations require careful attention to broadcasting weights and avoiding driver-only updates.\n",
    "- Parsing and managing numerical data from text-based CSV format required custom parsing and float conversion.\n",
    "- Ensuring the sigmoid function didnâ€™t overflow required care with extreme dot product values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
